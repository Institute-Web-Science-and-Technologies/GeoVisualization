

\subsection{Serverseitige und clientseitige Logik}


Um den Zustand der einzelnen mobilen Geräte zu synchronisieren und Chat-Nachrichten zu übertragen, ist ein Server nötig.
Hierzu bieten sich zwei unterschiedliche Modelle an.
Bei der klassischen Server-Client-Architektur übernimmt der Server einen Großteil der Berechnungen.
Der Server hält einen zentralen, im Zweifelsfall gültigen, Status.
Die (Thin-)Clients übertragen die Nutzereingaben und Sensordaten an den Server, der daraus einen neuen Zustand ermittelt und diesen den Clients mitteilt.

Alternativ kann man die Logik vorrangig clientseitig implementieren. 
Dabei müssen die (Fat-)Clients  den Großteil der Berechnungen übernehmen. Dies fordert leistungsfähigere Endgeräte, während der Server entlastet wird. 
Aktuelle Smartphones könnten die entwickelte Anwendung jedoch mühelos ausführen.
%Diese haben sich als genügend leistungsfähig erwiesen.
Bei fast jeder Änderung muss jeder Client seinen Zustand selbst\-stän\-dig aktualisieren, da die Programmlogik redundant auf jedem Client vorliegt. Dies verringert theoretisch Wartbarkeit und Erweiterbarkeit. Da die Implementation definitiv eine Internetverbindung benötigt, könnte theoretisch der Server mit einer Updatefunktionalität versehen werden. Auch könnte dieses Problem, bei entsprechender Produktreife, über die Aktualisierungsmöglichkeit von App-Stores gelöst werden. In Mehrspieleranwendungen kommt zusätzlich hinzu, dass es keinen definitiven, zentralen Zustand gibt und auf verschiedenen Clients zum gleichen Zeitpunkt unterschiedliche Zustände vorliegen können. Da diese Zustände für Berechnungen genutzt werden, kann dies zu Unklarheiten und Fehlern führen, die abgefangen werden müssen.
%Wir haben abgeschätzt, dass wir mit Fat-Clients Internet-Bandbreite sparen können, da kleinere Daten übertragen werden müssen. 
Im Fall der Implementation würde bei einer Thin-Client-Architektur um einiges mehr Informationen hin und her geschickt werden. Der Server müsste z.B. jeweils auf die aktuellen Spielinformationen (z.B. Positionsdaten) von allen Clients warten, Tests haben gezeigt das die Updateintervalle von Gerät zu Gerät unterschiedlich sind und zum teil durch äußere Einflüsse (z.B. Wetter) verzögert werden können. Aus allen Updates berechnet der Server dann den aktuellen Spiel-Zustand, welcher wieder an alle Geräte übertragen werden muss. Hierbei muss die korrekte Übertragung sichergestellt sein. Nun würde der Zyklus wieder von vorne beginnen. Wenn man jetzt noch etwaige Verbindungsprobleme berücksichtigt wäre die Spielgeschwindigkeit überaus langsam. 

Bei einer Fat-Client-Architektur hingegen würden Clients die Spielinformationen schnellst möglich an den Server schicken, welcher diese nur weiterleiten muss. Wenn es zu Übertragungsverzögerungen kommt, können ein- und ausgehende Spielinformationen in Warteschleifen platziert werden. 
Im implementierten Capture-the-Flag-Spielmodus muss beispielsweise bloß der aktuelle Standpunkt eines Spielers übertragen werden, nicht die daraus resultierenden Daten, wie die Sichtbarkeit für andere Spieler, die der Fat-Client selber berechnet.
Die Spielgeschwindigkeit wäre also nur abhängig von der Rechen- bzw. Sensorleistung der Clients, sowie der möglichen Übertragungsgeschwindigkeit. Durch Verzögerungen kann es zwar immer noch für den Endnutzer zu unvorhergesehen oder falsch erscheinenden Interaktionsergebnissen kommen, z.B. wer bei der Snake-Implementation mit wem kollidiert, ist aber in unserer Implementation zu vernachlässigen. Es wird vor allem Wert auf Spielgeschwindigkeit gelegt. Dieser Punkt war auch ausschlaggebend dafür, das in diesem Projekt eine Fat-Client-Architektur gewählt wurde\cite{schaeffer}.

\begin{center}
\begin{tabular}{l|l}
	Thin-Client & Fat-Client \\
	\hline
	 - Datenübertragung & + Datenübertragung \\
	 - Zustand doppelt modelliert & + Server-Zustand nicht nötig \\
	 + zentraler, definitiver Zustand &  - verteilter Zustand \\
	 (+) Wartbarkeit & (-) Wartbarkeit \\
	 (+) Erweiterbarkeit & (-) Erweiterbarkeit \\
	 - Spielgeschwindigkeit & + Spielgeschwindigkeit \\
\end{tabular}
\end{center}

\subsection{Publish-Subscribe-Pattern}
Das Publish-Subscribe-Pattern ist ein Entwurfsmuster für Server-Client -Kommunikation. Es gleicht dem Beobachter-Muster (\textit{observer pattern}).
Die Kommunikation zwischen Server und Client wird dabei als Push"=Kommunikation umgesetzt:
Die Clients nehmen die Rolle des Subscribers ein. Jeder Client meldet sich beim Server, der in der Rolle des Publishers fungiert, für bestimmte Typen von Nachrichten an. Tritt ein bestimmtes Ereignis auf, beispielsweise eine Änderung des Server-Zustands, informiert der Server aktiv alle Clients, die diese Art Ereignis abonniert haben, und schickt jedem eine Nachricht \cite{xmpp, eugster}.

Im Gegensatz dazu wird bei der Pull-Kommunikation der Server ausschließlich auf Anfrage eines Clients aktiv. Falls der Client nicht bloß einzelne Berechnungen auf den Server auslagern soll, sondern Informationen über externe Ereignisse braucht, muss er in regelmäßigen Abständen diese Informationen beim Server anfragen. 
Dies verbraucht zusätzliche Bandbreite und führt zu einer Verzögerung der Aktualisierung des Client-Zustands. Da die Bandbreite bei mobilen Geräten von vielen Faktoren negativ beeinflusst werden kann und Verzögerungen bei einem interaktiven Spiel stören können, wurden beide Faktoren als kritisch eingestuft. Daher wurde Push-Kommunikation verwendet. 

Die Kommunikation zwischen Server und Client haben wir mit Hilfe von \textit{message passing} realisiert. \textit{Message passing} ist eine Variante des Publish"=Subscribe"=Patterns, bei dem der Server keinen Zustand hält, sondern bloß eingehende Nachrichten an alle Clients weiterleitet, die den entsprechen Nachrichtentyp abonniert haben.
Dabei werden alle Berechnungen vom Client ausgeführt.
Dadurch verringert sich die Komplexität der Anwendung, da der Zustand nicht doppelt modelliert werden muss. Dies macht die Anwendung weniger fehleranfällig \cite{eugster}.